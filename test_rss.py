import feedparser
import json
from datetime import datetime

def test_rss_sources():
    """æµ‹è¯•RSSæºçš„æŠ“å–åŠŸèƒ½"""
    
    # è¯»å–RSSæº
    try:
        with open('rss_sources.txt', 'r', encoding='utf-8') as f:
            rss_sources = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print("âŒ æ‰¾ä¸åˆ° rss_sources.txt æ–‡ä»¶")
        return
    
    print(f"ğŸ“¡ å¼€å§‹æµ‹è¯• {len(rss_sources)} ä¸ªRSSæº...")
    print("=" * 50)
    
    total_articles = 0
    all_articles = []  # å­˜å‚¨æ‰€æœ‰æ–‡ç« çš„å®Œæ•´ä¿¡æ¯
    
    for i, rss_url in enumerate(rss_sources, 1):
        print(f"\nğŸ” æµ‹è¯•æº {i}: {rss_url}")
        
        try:
            # è§£æRSS
            feed = feedparser.parse(rss_url)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè§£æ
            if feed.bozo:
                print(f"âš ï¸  RSSè§£ææœ‰è­¦å‘Š: {feed.bozo_exception}")
            
            # è·å–Feedä¿¡æ¯
            feed_title = feed.feed.get('title', 'æœªçŸ¥æ ‡é¢˜')
            feed_desc = feed.feed.get('description', 'æ— æè¿°')
            entries_count = len(feed.entries)
            
            print(f"âœ… Feedæ ‡é¢˜: {feed_title}")
            print(f"ğŸ“ Feedæè¿°: {feed_desc[:100]}{'...' if len(feed_desc) > 100 else ''}")
            print(f"ğŸ“° æ–‡ç« æ•°é‡: {entries_count}")
            
            total_articles += entries_count
            
            # æ˜¾ç¤ºå‰3ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ§åˆ¶å°é¢„è§ˆï¼‰
            print(f"ğŸ“‹ å‰3ç¯‡æ–‡ç« é¢„è§ˆ:")
            for j, entry in enumerate(feed.entries[:3], 1):
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', 'æ— é“¾æ¥')
                summary = entry.get('summary', 'æ— æ‘˜è¦')
                published = entry.get('published', 'æ— å‘å¸ƒæ—¶é—´')
                
                print(f"  {j}. æ ‡é¢˜: {title}")
                print(f"     é“¾æ¥: {link}")
                print(f"     æ‘˜è¦: {summary[:150]}{'...' if len(summary) > 150 else ''}")
                print(f"     å‘å¸ƒ: {published}")
                print()
            
            # ä¿å­˜æ‰€æœ‰æ–‡ç« çš„å®Œæ•´ä¿¡æ¯
            for entry in feed.entries:
                article = {
                    'rss_source': rss_url,
                    'rss_title': feed_title,
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'summary': entry.get('summary', ''),
                    'content': entry.get('content', [{}])[0].get('value', '') if entry.get('content') else '',
                    'published': entry.get('published', ''),
                    'published_parsed': str(entry.get('published_parsed', '')),
                    'author': entry.get('author', ''),
                    'tags': [tag.get('term', '') for tag in entry.get('tags', [])],
                    'id': entry.get('id', ''),
                    'guid': entry.get('guid', '')
                }
                all_articles.append(article)
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
    
    print("=" * 50)
    print(f"ğŸ¯ æµ‹è¯•å®Œæˆï¼æ€»å…±è·å–åˆ° {total_articles} ç¯‡æ–‡ç« ")
    
    # ä¿å­˜å®Œæ•´æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶
    test_result = {
        'test_time': datetime.now().isoformat(),
        'total_sources': len(rss_sources),
        'total_articles': total_articles,
        'sources': rss_sources,
        'articles': all_articles
    }
    
    with open('rss_test_result.json', 'w', encoding='utf-8') as f:
        json.dump(test_result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ å®Œæ•´æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° rss_test_result.json")
    
    # é¢å¤–ä¿å­˜ä¸€ä¸ªçº¯æ–‡æœ¬æ ¼å¼çš„æ–‡ç« åˆ—è¡¨ï¼Œä¾¿äºé˜…è¯»
    with open('rss_articles.txt', 'w', encoding='utf-8') as f:
        f.write(f"RSSæ–‡ç« æŠ“å–ç»“æœ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        for i, article in enumerate(all_articles, 1):
            f.write(f"æ–‡ç«  {i}:\n")
            f.write(f"æ¥æº: {article['rss_title']} ({article['rss_source']})\n")
            f.write(f"æ ‡é¢˜: {article['title']}\n")
            f.write(f"é“¾æ¥: {article['link']}\n")
            f.write(f"å‘å¸ƒæ—¶é—´: {article['published']}\n")
            f.write(f"ä½œè€…: {article['author']}\n")
            f.write(f"æ ‡ç­¾: {', '.join(article['tags'])}\n")
            f.write(f"æ‘˜è¦: {article['summary']}\n")
            if article['content']:
                f.write(f"æ­£æ–‡: {article['content']}\n")
            f.write("-" * 80 + "\n\n")
    
    print(f"ğŸ“ æ–‡ç« å†…å®¹å·²ä¿å­˜åˆ° rss_articles.txt")
    print(f"ğŸ’¾ å…±ä¿å­˜äº† {len(all_articles)} ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯")

if __name__ == '__main__':
    test_rss_sources() 